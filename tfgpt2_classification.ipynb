{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tfgp2_classification.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM+/XHa16qVdU3Y6sLCqNhU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HyeonhoonLee/NIPA2020/blob/main/tfgpt2_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iA2SflACXMIs"
      },
      "source": [
        "# !pip install tensorflow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQeuMdxcXhco"
      },
      "source": [
        "# 1. EDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xt_yLosSXQli"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "import sklearn \n",
        "from sklearn import model_selection\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXEnFJz_XQoC"
      },
      "source": [
        "os.getcwd()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5BjLnm_TXQqe"
      },
      "source": [
        "# DATA_IN_PATH = '/home/workspace/data/.train/.task156/data/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2iJT77eqXQut"
      },
      "source": [
        "print(\"파일 크기 : \")\n",
        "for file in os.listdir(DATA_IN_PATH):\n",
        "    if 'tsv' in file :\n",
        "        print(file.ljust(30) + str(round(os.path.getsize(DATA_IN_PATH + file) / 1000000, 2)) + 'MB')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUEzL69JXQw3"
      },
      "source": [
        "# randsom seed\n",
        "seed=1234\n",
        "np.random.seed(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KChwMx3RXQ00"
      },
      "source": [
        "num_classes=4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eIoSnGgsXdSY"
      },
      "source": [
        "#loading csv data\n",
        "all_data = pd.read_csv(DATA_IN_PATH + 'train.tsv', sep = '\\t', quoting = 2)\n",
        "all_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ajgoDGnxXdUZ"
      },
      "source": [
        "# all_data의 전체 길이\n",
        "len(all_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WgQCCtpfXdYO"
      },
      "source": [
        "all_data['tag'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VK2Ce8cXdca"
      },
      "source": [
        "# tag format\n",
        "# 0(음악), 1(퀴즈), 2(사연), 3(수다)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ol4byvgVXdew"
      },
      "source": [
        "#stratififed split to train_data, validate_data\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=seed)\n",
        "\n",
        "for train_idx, validate_idx in split.split(all_data, all_data[\"tag\"]):\n",
        "    train_data = all_data.loc[train_idx]\n",
        "    validate_data = all_data.loc[validate_idx]\n",
        "\n",
        "print('전체 학습데이터의 개수: {}'.format(len(train_data)))\n",
        "print('전체 학습데이터의 개수: {}'.format(len(validate_data)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1oBuFFeXdgl"
      },
      "source": [
        "train_length = train_data['comment'].astype(str).apply(len)\n",
        "train_length.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u42HbbgwXdiv"
      },
      "source": [
        "# 그래프에 대한 이미지 사이즈 선언\n",
        "# figsize: (가로, 세로) 형태의 튜플로 입력\n",
        "plt.figure(figsize=(12, 5))\n",
        "# 히스토그램 선언\n",
        "# bins: 히스토그램 값들에 대한 버켓 범위\n",
        "# range: x축 값의 범위\n",
        "# alpha: 그래프 색상 투명도\n",
        "# color: 그래프 색상\n",
        "# label: 그래프에 대한 라벨\n",
        "plt.hist(train_length, bins=200, alpha=0.5, color= 'r', label='word')\n",
        "plt.yscale('log', nonposy='clip')\n",
        "# 그래프 제목\n",
        "plt.title('Log-Histogram of length of comment')\n",
        "# 그래프 x 축 라벨\n",
        "plt.xlabel('Length of comment')\n",
        "# 그래프 y 축 라벨\n",
        "plt.ylabel('Number of comment')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Hvyjdt9Xdkz"
      },
      "source": [
        "print('comment 길이 최대 값: {}'.format(np.max(train_length)))\n",
        "print('comment 길이 최소 값: {}'.format(np.min(train_length)))\n",
        "print('comment 길이 평균 값: {:.2f}'.format(np.mean(train_length)))\n",
        "print('comment 길이 표준편차: {:.2f}'.format(np.std(train_length)))\n",
        "print('comment 길이 중간 값: {}'.format(np.median(train_length)))\n",
        "# 사분위의 대한 경우는 0~100 스케일로 되어있음\n",
        "print('comment 길이 제 1 사분위: {}'.format(np.percentile(train_length, 25)))\n",
        "print('comment 길이 제 3 사분위: {}'.format(np.percentile(train_length, 75)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjqx-b_oXdo5"
      },
      "source": [
        "plt.figure(figsize=(12, 5))\n",
        "# 박스플롯 생성\n",
        "# 첫번째 파라메터: 여러 분포에 대한 데이터 리스트를 입력\n",
        "# labels: 입력한 데이터에 대한 라벨\n",
        "# showmeans: 평균값을 마크함\n",
        "\n",
        "plt.boxplot(train_length,\n",
        "             labels=['counts'],\n",
        "             showmeans=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O6mpJ_x9XdrS"
      },
      "source": [
        "# 보통은 문자열이 아닌 데이터는 삭제한다.. 숫자도 필요없을듯\n",
        "train_comment = [comment for comment in train_data['comment'] if type(comment) is str]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TxYWtTovXdto"
      },
      "source": [
        "fig, axe = plt.subplots(ncols=1)\n",
        "fig.set_size_inches(20, 3)\n",
        "sns.countplot(train_data['tag'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cyaTcUu1Xdv8"
      },
      "source": [
        "for i in range(num_classes):\n",
        "  print(\"comment 개수: {}\".format(train_data['tag'].value_counts()[i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BZjvnbR-Xdx8"
      },
      "source": [
        "# 각 comment 서술의 단어 수를 확인.\n",
        "# 데이터를 띄어쓰기 기준으로 나눠서 그 개수를 하나의 변수로 할당한다.\n",
        "train_word_counts = train_data['comment'].astype(str).apply(lambda x:len(x.split(' ')))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yQCIBwh6X-oX"
      },
      "source": [
        "plt.figure(figsize=(15, 10))\n",
        "plt.hist(train_word_counts, bins=50, facecolor='r',label='train')\n",
        "plt.title('Log-Histogram of word count in comment', fontsize=15)\n",
        "plt.yscale('log', nonposy='clip')\n",
        "plt.legend()\n",
        "plt.xlabel('Number of comment', fontsize=15)\n",
        "plt.ylabel('Number of comment', fontsize=15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yatKRlFAX-qh"
      },
      "source": [
        "print('comment 단어 개수 최대 값: {}'.format(np.max(train_word_counts)))\n",
        "print('comment 단어 개수 최소 값: {}'.format(np.min(train_word_counts)))\n",
        "print('comment 단어 개수 평균 값: {:.2f}'.format(np.mean(train_word_counts)))\n",
        "print('comment 단어 개수 표준편차: {:.2f}'.format(np.std(train_word_counts)))\n",
        "print('comment 단어 개수 중간 값: {}'.format(np.median(train_word_counts)))\n",
        "# 사분위의 대한 경우는 0~100 스케일로 되어있음\n",
        "print('comment 단어 개수 제 1 사분위: {}'.format(np.percentile(train_word_counts, 25)))\n",
        "print('comment 단어 개수 제 3 사분위: {}'.format(np.percentile(train_word_counts, 75)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nA9T8xIyX-sf"
      },
      "source": [
        "# 특수문자 확인하기\n",
        "# 본 데이터는 크롤링 과정에서 이모티콘들이 모두 ? 로 처리되었으므로 주의\n",
        "qmarks = np.mean(train_data['comment'].astype(str).apply(lambda x: '?' in x)) # 물음표가 구두점으로 쓰임\n",
        "fullstop = np.mean(train_data['comment'].astype(str).apply(lambda x: '.' in x)) # 마침표\n",
        "                  \n",
        "print('물음표가있는 질문: {:.2f}%'.format(qmarks * 100))\n",
        "print('마침표가 있는 질문: {:.2f}%'.format(fullstop * 100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qDs9W8dkYEbY"
      },
      "source": [
        "# 2. Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ii8t8NZaX-uY"
      },
      "source": [
        "# 기본 데이터\n",
        "train_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kr3h5hgvYJmt"
      },
      "source": [
        "## 2.1. 특수 문자 및 불용어 제거"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8kT8yINoX-wi"
      },
      "source": [
        "# 물음표, 마침표 등을 지움.\n",
        "import string\n",
        "string.punctuation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMnrkwbRX-yw"
      },
      "source": [
        "def remove_punctuation(text):\n",
        "    no_punct=[words for words in text if words not in string.punctuation]\n",
        "    words_wo_punct=''.join(no_punct)\n",
        "    return words_wo_punct\n",
        "train_data['comment_clean']=train_data['comment'].apply(lambda x: remove_punctuation(x))\n",
        "train_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-6ZrdHeX-0s"
      },
      "source": [
        "#여기는 \"띄워쓰기가 가능한 어절(단어)\" 중에서, 분류에 영향을 미치지 않을 단어들에 대해서 작성!\n",
        "stopwords = ['안녕하세요']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "liXQ2ND1X-4y"
      },
      "source": [
        "def remove_stopwords(text):\n",
        "    splited = text.split(\" \")\n",
        "    no_stop=[words for words in splited if words not in stopwords]\n",
        "    words_wo_stop=' '.join(no_stop)\n",
        "    return words_wo_stop\n",
        "train_data['comment_clean']=train_data['comment_clean'].apply(lambda x: remove_stopwords(x))\n",
        "train_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2B5AjzTjX-69"
      },
      "source": [
        "#여기는 \"음절\" 중에서, 분류에 영향을 미치지 않을 단어들에 대해서 작성!\n",
        "#단, '음절'이기 때문에 우리에게 필요한 어절(단어)에 들어있는 음절도 지워지게 되면 오히려 악영향을 미칠 수 있으므로, 확실하게 필요없는 것만 작성!\n",
        "stopletters = ['ㅋ', 'ㅇㅈ', 'ㅎ', '...', 'ㅗ', 'ㄷ']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3eVeoc1YWwR"
      },
      "source": [
        "def remove_stopletters(text):\n",
        "    no_letter=[words for words in text if words not in stopletters]\n",
        "    words_wo_stop=''.join(no_letter)\n",
        "    return words_wo_stop\n",
        "train_data['comment_clean']=train_data['comment_clean'].apply(lambda x: remove_stopletters(x))\n",
        "train_data.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzgiUH60YbWS"
      },
      "source": [
        "## 2.2. Resampling (over/under sampling)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHLSBcQ7YWyc"
      },
      "source": [
        "train_data.tag.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUG2whKaYW0a"
      },
      "source": [
        "# Class count\n",
        "count_class_3, count_class_1, count_class_0, count_class_2 = train_data.tag.value_counts()\n",
        "\n",
        "# Divide by class\n",
        "df_class_0 = train_data[train_data['tag'] == 0]\n",
        "df_class_1 = train_data[train_data['tag'] == 1]\n",
        "df_class_2 = train_data[train_data['tag'] == 2]\n",
        "df_class_3 = train_data[train_data['tag'] == 3]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3eAIVBnYiWN"
      },
      "source": [
        "# random oversampling (0,2 to the number of 1)\n",
        "df_class_0_over = df_class_0.sample(count_class_1, replace=True)\n",
        "df_class_2_over = df_class_2.sample(count_class_1, replace=True)\n",
        "df_test_over = pd.concat([df_class_0_over, df_class_1, df_class_2_over, df_class_3], axis=0)\n",
        "\n",
        "print('Random over-sampling:')\n",
        "print(df_test_over.tag.value_counts())\n",
        "\n",
        "df_test_over.tag.value_counts().plot(kind='bar', title='Count (tag)');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RSL5qTnYYiY6"
      },
      "source": [
        "df_test_over.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LNODF6JYsE7"
      },
      "source": [
        "## 3. GPT2를 이용한 텍스트 분류"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n8sjjJMrYibO"
      },
      "source": [
        "# !pip install gluonnlp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lo1bDKtYidb"
      },
      "source": [
        "# # installing transforemrs\n",
        "# !pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8LCT0p-Yifg"
      },
      "source": [
        "# !wget https://www.dropbox.com/s/nzfa9xpzm4edp6o/gpt_ckpt.zip -O gpt_ckpt.zip\n",
        "# !unzip -o gpt_ckpt.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_Od0ePbYiha"
      },
      "source": [
        "# !pip install mxnet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJv3GbkAYzCb"
      },
      "source": [
        "import tensorflow as tf\n",
        "import gluonnlp as nlp\n",
        "from gluonnlp.data import SentencepieceTokenizer\n",
        "import re\n",
        "from transformers import TFGPT2Model\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "\n",
        "import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-17W_dCpYzEQ"
      },
      "source": [
        "# 시각화\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "    plt.plot(history.history[string])\n",
        "    plt.plot(history.history['val_'+string], '')\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(string)\n",
        "    plt.legend([string, 'val_'+string])\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHyHPyPDYzGH"
      },
      "source": [
        "SEED_NUM = 1234\n",
        "tf.random.set_seed(SEED_NUM)\n",
        "np.random.seed(SEED_NUM)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtTR98rmYzIL"
      },
      "source": [
        "TOKENIZER_PATH = './gpt_ckpt/gpt2_kor_tokenizer.spiece'\n",
        "\n",
        "tokenizer = SentencepieceTokenizer(TOKENIZER_PATH)\n",
        "vocab = nlp.vocab.BERTVocab.from_sentencepiece(TOKENIZER_PATH,\n",
        "                                               mask_token=None,\n",
        "                                               sep_token='<unused0>',\n",
        "                                               cls_token=None,\n",
        "                                               unknown_token='<unk>',\n",
        "                                               padding_token='<pad>',\n",
        "                                               bos_token='<s>',\n",
        "                                               eos_token='</s>')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSiAiPT5YzLq"
      },
      "source": [
        "BATCH_SIZE = 16\n",
        "NUM_EPOCHS = 20\n",
        "# VALID_SPLIT = 0.1\n",
        "SENT_MAX_LEN = 39\n",
        "\n",
        "DATA_IN_PATH = '/home/workspace/data/.train/.task156/data/'\n",
        "DATA_OUT_PATH = '/home/workspace/user-workspace/sbs'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N61EfHISYzNp"
      },
      "source": [
        "# 텍스트 전처리\n",
        "## 여기 단게에서 해도 된다!\n",
        "\n",
        "def clean_text(sent):\n",
        "    sent_clean = re.sub(\"[^가-힣ㄱ-ㅎㅏ-ㅣ\\\\s]\", \"\", sent)\n",
        "    return sent_clean"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VfcU2Ko2YzPn"
      },
      "source": [
        "# train_data = train_data[:50] # for test\n",
        "\n",
        "train_data_sents = []\n",
        "train_data_labels = []\n",
        "\n",
        "# for train_sent, train_label in df_test_over[['comment_wo_punct', 'tag']].values:\n",
        "for train_sent, train_label in df_test_over[['comment_clean', 'tag']].values:\n",
        "    train_tokenized_text = vocab[tokenizer(clean_text(train_sent))]\n",
        "\n",
        "    tokens = [vocab[vocab.bos_token]]  \n",
        "    tokens += pad_sequences([train_tokenized_text], \n",
        "                            SENT_MAX_LEN, \n",
        "                            value=vocab[vocab.padding_token], \n",
        "                            padding='post').tolist()[0] \n",
        "    tokens += [vocab[vocab.eos_token]]\n",
        "\n",
        "    train_data_sents.append(tokens)\n",
        "    train_data_labels.append(train_label)\n",
        "\n",
        "train_data_sents = np.array(train_data_sents, dtype=np.int64)\n",
        "train_data_labels = np.array(train_data_labels, dtype=np.int64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8miB5pYXYzTh"
      },
      "source": [
        "validate_data.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYrwjZnWYzKS"
      },
      "source": [
        "# 문자열로 변환해줌.\n",
        "validate_data['comment'].astype(str)\n",
        "validate_data['tag'].astype(str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9UNjJG03YzRV"
      },
      "source": [
        "# 결측치를 문자열로 채워줌.\n",
        "validate_data = validate_data.fillna('결측값')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERxdYQOYZPbv"
      },
      "source": [
        "validate_data.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iG__iFbrZPdz"
      },
      "source": [
        "# validate_data\n",
        "\n",
        "validate_data_sents = []\n",
        "validate_data_labels = []\n",
        "\n",
        "for validate_sent, validate_label in validate_data[['comment', 'tag']].values:\n",
        "    validate_tokenized_text = vocab[tokenizer(validate_sent)]\n",
        "\n",
        "    tokens = [vocab[vocab.bos_token]]  \n",
        "    tokens += pad_sequences([validate_tokenized_text], \n",
        "                            SENT_MAX_LEN, \n",
        "                            value=vocab[vocab.padding_token], \n",
        "                            padding='post').tolist()[0] \n",
        "    tokens += [vocab[vocab.eos_token]]\n",
        "\n",
        "    validate_data_sents.append(tokens)\n",
        "    validate_data_labels.append(validate_label)\n",
        "\n",
        "validate_data_sents = np.array(validate_data_sents, dtype=np.int64)\n",
        "validate_data_labels = np.array(validate_data_labels, dtype=np.int64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aC4Y3EWZPf_"
      },
      "source": [
        "class TFGPT2Classifier(tf.keras.Model):\n",
        "    def __init__(self, dir_path, num_class):\n",
        "        super(TFGPT2Classifier, self).__init__()\n",
        "        \n",
        "        self.gpt2 = TFGPT2Model.from_pretrained(dir_path)\n",
        "        self.num_class = num_class\n",
        "        \n",
        "        self.dropout = tf.keras.layers.Dropout(self.gpt2.config.summary_first_dropout)\n",
        "        self.classifier = tf.keras.layers.Dense(self.num_class, \n",
        "                                                kernel_initializer=tf.keras.initializers.TruncatedNormal(stddev=self.gpt2.config.initializer_range), \n",
        "                                                name=\"classifier\")\n",
        "        \n",
        "    def call(self, inputs):\n",
        "        outputs = self.gpt2(inputs)\n",
        "        pooled_output = outputs[0][:, -1]\n",
        "\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzI5g2S3ZPiM"
      },
      "source": [
        "BASE_MODEL_PATH = './gpt_ckpt'\n",
        "cls_model = TFGPT2Classifier(dir_path=BASE_MODEL_PATH, num_class=num_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-uRa5yIZPkh"
      },
      "source": [
        "def recall(y_target, y_pred):\n",
        "    # clip(t, clip_value_min, clip_value_max) : clip_value_min~clip_value_max 이외 가장자리를 깎아 낸다\n",
        "    # round : 반올림한다\n",
        "    y_target_yn = tf.round(tf.keras.backend.clip(y_target, 0, 1)) # 실제값을 0(Negative) 또는 1(Positive)로 설정한다\n",
        "    y_pred_yn = tf.round(tf.keras.backend.clip(y_pred, 0, 1)) # 예측값을 0(Negative) 또는 1(Positive)로 설정한다\n",
        "\n",
        "    # True Positive는 실제 값과 예측 값이 모두 1(Positive)인 경우이다\n",
        "    count_true_positive = tf.keras.backend.sum(y_target_yn * y_pred_yn) \n",
        "\n",
        "    # (True Positive + False Negative) = 실제 값이 1(Positive) 전체\n",
        "    count_true_positive_false_negative = tf.keras.backend.sum(y_target_yn)\n",
        "\n",
        "    # Recall =  (True Positive) / (True Positive + False Negative)\n",
        "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
        "    recall = count_true_positive / (count_true_positive_false_negative + tf.keras.backend.epsilon())\n",
        "\n",
        "    # return a single tensor value\n",
        "    return recall\n",
        "\n",
        "def precision(y_target, y_pred):\n",
        "    # clip(t, clip_value_min, clip_value_max) : clip_value_min~clip_value_max 이외 가장자리를 깎아 낸다\n",
        "    # round : 반올림한다\n",
        "    y_pred_yn = tf.round(tf.keras.backend.clip(y_pred, 0, 1)) # 예측값을 0(Negative) 또는 1(Positive)로 설정한다\n",
        "    y_target_yn = tf.round(tf.keras.backend.clip(y_target, 0, 1)) # 실제값을 0(Negative) 또는 1(Positive)로 설정한다\n",
        "\n",
        "    # True Positive는 실제 값과 예측 값이 모두 1(Positive)인 경우이다\n",
        "    count_true_positive = tf.keras.backend.sum(y_target_yn * y_pred_yn) \n",
        "\n",
        "    # (True Positive + False Positive) = 예측 값이 1(Positive) 전체\n",
        "    count_true_positive_false_positive = tf.keras.backend.sum(y_pred_yn)\n",
        "\n",
        "    # Precision = (True Positive) / (True Positive + False Positive)\n",
        "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
        "    precision = count_true_positive / (count_true_positive_false_positive + tf.keras.backend.epsilon())\n",
        "\n",
        "    # return a single tensor value\n",
        "    return precision\n",
        "\n",
        "def f1score(y_target, y_pred):\n",
        "    _recall = recall(y_target, y_pred)\n",
        "    _precision = precision(y_target, y_pred)\n",
        "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
        "    _f1score = ( 2 * _recall * _precision) / (_recall + _precision+ tf.keras.backend.epsilon())\n",
        "    \n",
        "    # return a single tensor value\n",
        "    return _f1score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYXHsB7wZPnL"
      },
      "source": [
        "# 학습 준비하기\n",
        "## f1 score 계산 및 label_smoothing을 위해 one-hot label을 사용하려면 categoricalcrossentropy를 사용해야함.\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=6e-6)\n",
        "# loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "# metric = tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy')\n",
        "loss = tf.keras.losses.CategoricalCrossentropy(from_logits=True, label_smoothing=0.0, reduction=tf.keras.losses.Reduction.NONE)\n",
        "metric = tf.keras.metrics.CategoricalAccuracy(name='accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9CBeb_hZZYO"
      },
      "source": [
        "# model compile\n",
        "cls_model.compile(optimizer=optimizer, loss=loss, metrics=[metric, f1score], experimental_run_tf_function=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "haZ-Vf36ZZaT"
      },
      "source": [
        "import datetime\n",
        "dt = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rt8_wi0WZZcY"
      },
      "source": [
        "# Callback checkpoint dir\n",
        "model_name = \"tf2_gpt2\" + str(dt)\n",
        "checkpoint_path = os.path.join(DATA_OUT_PATH, model_name, 'weights.h5')\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "# Create path if exists\n",
        "if os.path.exists(checkpoint_dir):\n",
        "    print(\"{} -- Folder already exists \\n\".format(checkpoint_dir))\n",
        "else:\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "    print(\"{} -- Folder create complete \\n\".format(checkpoint_dir))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gX_v6Z6NZZgX"
      },
      "source": [
        "# overfitting을 막기 위한 earlystop 추가\n",
        "earlystop_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0001,patience=5, mode='min')\n",
        "# min_delta: the threshold that triggers the termination (acc should at least improve 0.0001)\n",
        "# patience: no improvment epochs (patience = 1, 1번 이상 상승이 없으면 종료)\\\n",
        "\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    checkpoint_path, monitor='val_loss', verbose=1, save_best_only=True, save_weights_only=True, mode='min')\n",
        "\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
        "    monitor='val_loss', factor=0.1, patience=3, verbose=0, mode='min',\n",
        "    min_delta=0.0001, cooldown=0, min_lr=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGJJwycKZZiv"
      },
      "source": [
        "# class_weight = {0.0:2.0, 1.0:1.0, 2.0:3.0, 3.0:1.0}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjN4iYITZZfK"
      },
      "source": [
        "from sklearn import preprocessing\n",
        "\n",
        "onehot_encoder = preprocessing.OneHotEncoder()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eb25VJS-ZsrJ"
      },
      "source": [
        "reshaped = train_data_labels.reshape(-1,1)\n",
        "train_data_labels_re = onehot_encoder.fit_transform(reshaped)\n",
        "train_data_labels_np = train_data_labels_re.toarray()\n",
        "train_data_labels_np = np.asarray(train_data_labels_np, dtype=np.int32)\n",
        "\n",
        "v_reshaped = validate_data_labels.reshape(-1,1)\n",
        "validate_data_labels_re = onehot_encoder.fit_transform(v_reshaped)\n",
        "validate_data_labels_np = validate_data_labels_re.toarray()\n",
        "validate_data_labels_np = np.asarray(validate_data_labels_np, dtype=np.int32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaD-iS3vZstP"
      },
      "source": [
        "# 학습과 eval 시작\n",
        "history = cls_model.fit(train_data_sents, train_data_labels_np, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE,\n",
        "                    validation_data=(validate_data_sents, validate_data_labels_np),\n",
        "                    validation_steps=(len(validate_data_labels_np)/BATCH_SIZE),\n",
        "                    callbacks=[cp_callback, earlystop_callback, reduce_lr])\n",
        "#                     class_weight=class_weight) \n",
        "#steps_for_epoch\n",
        "print(history.history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCQU4wcCZsvg"
      },
      "source": [
        "plot_graphs(history, 'accuracy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67Xw8xqfZszr"
      },
      "source": [
        "plot_graphs(history, 'loss')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUCZP-TIZsxh"
      },
      "source": [
        "plot_graphs(history, 'f1score')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNc46LgnZ2u-"
      },
      "source": [
        "## 4. Prediction and submission"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvHuqnMDZ18h"
      },
      "source": [
        "## To load a best weights from a saved file (.h5)\n",
        "cls_model.load_weights(checkpoint_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fO-DgmIZPpR"
      },
      "source": [
        "test_data = pd.read_csv(DATA_IN_PATH + 'test.tsv', sep = '\\t', quoting = 3)\n",
        "test_data.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hpFeOwz5Z7iy"
      },
      "source": [
        "total_number = len(test_data[\"comment\"])\n",
        "total_number"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ioPYBoP5Z7lK"
      },
      "source": [
        "# for test\n",
        "\n",
        "test_data_sents = []\n",
        "\n",
        "for test_sent in test_data['comment'].values:\n",
        "    test_tokenized_text = vocab[tokenizer(test_sent)]\n",
        "\n",
        "    tokens = [vocab[vocab.bos_token]]  \n",
        "    tokens += pad_sequences([test_tokenized_text], \n",
        "                            SENT_MAX_LEN, \n",
        "                            value=vocab[vocab.padding_token], \n",
        "                            padding='post').tolist()[0] \n",
        "    tokens += [vocab[vocab.eos_token]]\n",
        "\n",
        "    test_data_sents.append(tokens)\n",
        "\n",
        "test_data_sents = np.array(test_data_sents, dtype=np.int64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MGG5likZ7nk"
      },
      "source": [
        "len(test_data_sents)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VeuN-2FPZ7pn"
      },
      "source": [
        "from tqdm import tqdm\n",
        "taglist = []\n",
        "test_length = len(test_data_sents)\n",
        "loop = test_length//BATCH_SIZE\n",
        "for i in tqdm(range(BATCH_SIZE)):\n",
        "    result1 = cls_model(test_data_sents[loop*i:(loop*(1+i))])\n",
        "    prob = tf.nn.softmax(result1, axis=-1)\n",
        "    y_tag = np.argmax(prob, axis=-1)\n",
        "    taglist.extend(y_tag)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "giLjC0kBZ7t6"
      },
      "source": [
        "len(taglist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eidp7PTNZ7sl"
      },
      "source": [
        "test_data['tag'] = taglist\n",
        "test_data.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ka5eToHaMVv"
      },
      "source": [
        "# tag format\n",
        "# 0(음악), 1(퀴즈), 2(사연), 3(수다)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xrsug5sSaMYk"
      },
      "source": [
        "test_data['tag'].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVC2GCcpaMbE"
      },
      "source": [
        "submission_data = test_data[\"tag\"]\n",
        "sub_df = submission_data.to_frame()\n",
        "sub_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNY4Z5j-aMd8"
      },
      "source": [
        "sub_df.value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SjHgXlLEaS9T"
      },
      "source": [
        "from nipa.taskSubmit import nipa_submit"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xw-j-V7TaTE_"
      },
      "source": [
        "team_id = \"1429\"\n",
        "task_no= \"156\"\n",
        "prediction_path = '/home/workspace/user-workspace/prediction/prediction156.tsv'\n",
        "save_path = os.path.join(DATA_OUT_PATH, model_name, 'prediction156.tsv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-zoO98zaTHD"
      },
      "source": [
        "sub_df.to_csv(prediction_path, index = False, header = False)\n",
        "sub_df.to_csv(save_path, index = False, header = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65YHoYQNaTDX"
      },
      "source": [
        "print(\"is file: \", os.path.isfile(prediction_path))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c2pnNeR6aZkt"
      },
      "source": [
        "nipa_submit(team_id=team_id,\n",
        "task_no=task_no,\n",
        "result=prediction_path\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}