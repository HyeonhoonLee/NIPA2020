{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "image classification with blending ensemble.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNdmeB8G6wLVbUk5QmroA3P",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HyeonhoonLee/NIPA2020/blob/main/image_classification_with_blending_ensemble.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O1yfhZonA5o9"
      },
      "source": [
        "## 1. First Impressions and Getting Tools Ready¶\n",
        "\n",
        "from PIL import ImageFile\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
        "\n",
        "# loading packages\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "#\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "\n",
        "#\n",
        "\n",
        "import seaborn as sns\n",
        "import plotly.express as px\n",
        "\n",
        "#\n",
        "\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "import math\n",
        "import time\n",
        "\n",
        "from tqdm import tqdm\n",
        "from tqdm.keras import TqdmCallback\n",
        "\n",
        "\n",
        "from pandas_summary import DataFrameSummary\n",
        "\n",
        "import warnings\n",
        "\n",
        "\n",
        "warnings.filterwarnings('ignore') # Disabling warnings for clearer outputs\n",
        "\n",
        "\n",
        "\n",
        "# seed_val = 42\n",
        "# random.seed(seed_val)\n",
        "# np.random.seed(seed_val)\n",
        "\n",
        "# Setting color palette.\n",
        "orange_black = [\n",
        "    '#fdc029', '#df861d', '#FF6347', '#aa3d01', '#a30e15', '#800000', '#171820'\n",
        "]\n",
        "\n",
        "# Setting plot styling.\n",
        "plt.style.use('ggplot')\n",
        "\n",
        "os.getcwd()\n",
        "\n",
        "# Setting file paths for our notebook:\n",
        "base_path = '/home/workspace/data/.train/.task148/'\n",
        "train_img_path = '/home/workspace/data/.train/.task148/data/train/images/'\n",
        "test_img_path = '/home/workspace/data/.train/.task148/data/test/images/'\n",
        "# img_stats_path = '/kaggle/input/melanoma2020imgtabular'\n",
        "train_custom_img_stats_path = '/home/workspace/data/.train/.task148/data/train/train_custom.csv'\n",
        "train_img_stats_path = '/home/workspace/data/.train/.task148/data/train/train.csv'\n",
        "test_img_stats_path = '/home/workspace/data/.train/.task148/data/test/test.csv'\n",
        "\n",
        "DATA_OUT_PATH = '/home/workspace/user-workspace/kb'\n",
        "\n",
        "## 2. Loading Data\n",
        "\n",
        "seed = 1234\n",
        "BATCH_SIZE = 32  \n",
        "LEARNING_RATE = 3e-4\n",
        "EPOCHS = 100\n",
        "num_classes=5\n",
        "img_size = 128\n",
        "\n",
        "train_df = pd.read_csv(train_img_stats_path)\n",
        "test_df = pd.read_csv(test_img_stats_path)\n",
        "\n",
        "# train_custom_df = pd.read_csv(train_custom_img_stats_path)\n",
        "# train_custom_df.head()\n",
        "\n",
        "train_df.head()\n",
        "\n",
        "def add_png(name):\n",
        "    added = name + \".png\"\n",
        "    return added\n",
        "\n",
        "train_df[\"ID\"] = train_df[\"ID\"].apply(add_png)\n",
        "train_df.head()\n",
        "\n",
        "# from sklearn import preprocessing \n",
        "# label_encoder = preprocessing.LabelEncoder() \n",
        "# onehot_encoder = preprocessing.OneHotEncoder()\n",
        "# train_y = label_encoder.fit_transform(info['class2'])\n",
        "# train_y = train_y.reshape(len(train_y), 1)\n",
        "# train_y = onehot_encoder.fit_transform(train_y)\n",
        "\n",
        "\n",
        "train_df = train_df.replace(['10_콘크리트외벽', '20_조적외벽', '30_판넬외벽', '40_유리외벽', '50_기타외벽'], [0, 1, 2, 3, 4])\n",
        "\n",
        "# train_df.Target = pd.factorize(train_df.Target)[0]\n",
        "# train_df.head(10)\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.layers import Dense, Activation, Flatten, Dropout, BatchNormalization, Input\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
        "from tensorflow.keras import regularizers, optimizers, Model\n",
        "\n",
        "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        rotation_range=10,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        # shear_range=0.2,\n",
        "        # zoom_range=0.2,\n",
        "        # rotation_range=15,\n",
        "        # width_shift_range=0.1,\n",
        "        # height_shift_range=0.1,\n",
        "        horizontal_flip=True,\n",
        "        validation_split=0.2\n",
        "        )\n",
        "\n",
        "train_generator=train_datagen.flow_from_dataframe(\n",
        "  dataframe=train_df,\n",
        "  directory=train_img_path,\n",
        "  x_col=\"ID\",\n",
        "  y_col=\"Target\",\n",
        "  # y_col=[\"Plant\",\"Disease\"],\n",
        "  subset=\"training\",\n",
        "  batch_size=BATCH_SIZE,\n",
        "  seed=seed,\n",
        "  shuffle=True,\n",
        "  class_mode=\"raw\",\n",
        "  target_size=(img_size,img_size))\n",
        "\n",
        "validation_generator=train_datagen.flow_from_dataframe(\n",
        "  dataframe=train_df,\n",
        "  directory=train_img_path,\n",
        "  x_col=\"ID\",\n",
        "  y_col=\"Target\",\n",
        "  subset=\"validation\",\n",
        "  batch_size=BATCH_SIZE,\n",
        "  seed=seed,\n",
        "  shuffle=True,\n",
        "  class_mode=\"raw\",\n",
        "  target_size=(img_size,img_size))\n",
        "\n",
        "# orig_model = tf.keras.applications.EfficientNetB7(include_top=False, weights='imagenet', pooling='avg')\n",
        "# inp = Input(shape = (img_size,img_size,3))\n",
        "# x = orig_model(inp)\n",
        "# # output1 = Dense(8, activation = 'sigmoid')(x)\n",
        "# # output2 = Dense(14, activation = 'sigmoid')(x)\n",
        "# # model = Model(inp,[output1,output2])\n",
        "# output = Dense(num_classes, activation='sigmoid')(x)\n",
        "# model = Model(inp, output)\n",
        "\n",
        "model_input = tf.keras.Input(shape=(img_size, img_size, 3),\n",
        "                             name='img_input')\n",
        "\n",
        "dummy = tf.keras.layers.Lambda(lambda x: x)(model_input)\n",
        "\n",
        "outputs = []\n",
        "\n",
        "x = tf.keras.applications.EfficientNetB3(include_top=False,\n",
        "                       weights='imagenet',\n",
        "                       input_shape=(img_size,img_size, 3),\n",
        "                       pooling='avg')(dummy)\n",
        "x = tf.keras.layers.Dense(num_classes, activation='sigmoid')(x)\n",
        "outputs.append(x)\n",
        "\n",
        "x = tf.keras.applications.EfficientNetB4(include_top=False,\n",
        "                       weights='imagenet',\n",
        "                       input_shape=(img_size, img_size, 3),\n",
        "                       pooling='avg')(dummy)\n",
        "x = tf.keras.layers.Dense(num_classes, activation='sigmoid')(x)\n",
        "outputs.append(x)\n",
        "\n",
        "x = tf.keras.applications.EfficientNetB5(include_top=False,\n",
        "                       weights='imagenet',\n",
        "                       input_shape=(img_size, img_size, 3),\n",
        "                       pooling='avg')(dummy)\n",
        "x = tf.keras.layers.Dense(num_classes, activation='sigmoid')(x)\n",
        "outputs.append(x)\n",
        "model = tf.keras.Model(model_input, outputs, name='aNetwork')\n",
        "\n",
        "model.summary()\n",
        "\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=False)\n",
        "adam = tf.keras.optimizers.Adam(\n",
        "    learning_rate=LEARNING_RATE, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,\n",
        "    name='Adam')\n",
        "\n",
        "\n",
        "def recall(y_target, y_pred):\n",
        "    # clip(t, clip_value_min, clip_value_max) : clip_value_min~clip_value_max 이외 가장자리를 깎아 낸다\n",
        "    # round : 반올림한다\n",
        "    y_target_yn = tf.round(tf.keras.backend.clip(y_target, 0, 1)) # 실제값을 0(Negative) 또는 1(Positive)로 설정한다\n",
        "    y_pred_yn = tf.round(tf.keras.backend.clip(y_pred, 0, 1)) # 예측값을 0(Negative) 또는 1(Positive)로 설정한다\n",
        "\n",
        "    # True Positive는 실제 값과 예측 값이 모두 1(Positive)인 경우이다\n",
        "    count_true_positive = tf.keras.backend.sum(y_target_yn * y_pred_yn) \n",
        "\n",
        "    # (True Positive + False Negative) = 실제 값이 1(Positive) 전체\n",
        "    count_true_positive_false_negative = tf.keras.backend.sum(y_target_yn)\n",
        "\n",
        "    # Recall =  (True Positive) / (True Positive + False Negative)\n",
        "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
        "    recall = count_true_positive / (count_true_positive_false_negative + tf.keras.backend.epsilon())\n",
        "\n",
        "    # return a single tensor value\n",
        "    return recall\n",
        "\n",
        "def precision(y_target, y_pred):\n",
        "    # clip(t, clip_value_min, clip_value_max) : clip_value_min~clip_value_max 이외 가장자리를 깎아 낸다\n",
        "    # round : 반올림한다\n",
        "    y_pred_yn = tf.round(tf.keras.backend.clip(y_pred, 0, 1)) # 예측값을 0(Negative) 또는 1(Positive)로 설정한다\n",
        "    y_target_yn = tf.round(tf.keras.backend.clip(y_target, 0, 1)) # 실제값을 0(Negative) 또는 1(Positive)로 설정한다\n",
        "\n",
        "    # True Positive는 실제 값과 예측 값이 모두 1(Positive)인 경우이다\n",
        "    count_true_positive = tf.keras.backend.sum(y_target_yn * y_pred_yn) \n",
        "\n",
        "    # (True Positive + False Positive) = 예측 값이 1(Positive) 전체\n",
        "    count_true_positive_false_positive = tf.keras.backend.sum(y_pred_yn)\n",
        "\n",
        "    # Precision = (True Positive) / (True Positive + False Positive)\n",
        "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
        "    precision = count_true_positive / (count_true_positive_false_positive + tf.keras.backend.epsilon())\n",
        "\n",
        "    # return a single tensor value\n",
        "    return precision\n",
        "\n",
        "def f1score(y_target, y_pred):\n",
        "    _recall = recall(y_target, y_pred)\n",
        "    _precision = precision(y_target, y_pred)\n",
        "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
        "    _f1score = ( 2 * _recall * _precision) / (_recall + _precision+ tf.keras.backend.epsilon())\n",
        "    \n",
        "    # return a single tensor value\n",
        "    return _f1score\n",
        "\n",
        "model.compile(optimizer=adam, \n",
        "              loss = [loss, loss, loss],\n",
        "              metrics=[tf.keras.metrics.SparseCategoricalAccuracy(name='accuracy'), f1score]\n",
        "              )\n",
        "\n",
        "import datetime\n",
        "dt = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "\n",
        "# Callback checkpoint dir\n",
        "model_name = \"tf2_kb\" + str(dt)\n",
        "checkpoint_path = os.path.join(DATA_OUT_PATH, model_name, 'weights.h5')\n",
        "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
        "\n",
        "# Create path if exists\n",
        "if os.path.exists(checkpoint_dir):\n",
        "    print(\"{} -- Folder already exists \\n\".format(checkpoint_dir))\n",
        "else:\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "    print(\"{} -- Folder create complete \\n\".format(checkpoint_dir))\n",
        "\n",
        "# overfitting을 막기 위한 earlystop 추가\n",
        "earlystop_callback = tf.keras.callbacks.EarlyStopping(monitor='val_dense_1_loss', min_delta=0.0001,patience=5, mode='min')\n",
        "# min_delta: the threshold that triggers the termination (acc should at least improve 0.0001)\n",
        "# patience: no improvment epochs (patience = 1, 1번 이상 상승이 없으면 종료)\\\n",
        "\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    checkpoint_path, monitor='val_dense_1_loss', verbose=1, save_best_only=True, save_weights_only=True, mode='min')\n",
        "\n",
        "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_dense_1_loss', patience=2)\n",
        "\n",
        "STEP_SIZE_TRAIN=train_generator.n//train_generator.batch_size\n",
        "STEP_SIZE_VALID=validation_generator.n//validation_generator.batch_size\n",
        "\n",
        "history = model.fit_generator(\n",
        "    # generator=generator_wrapper(train_generator),\n",
        "    train_generator,\n",
        "    steps_per_epoch=STEP_SIZE_TRAIN,\n",
        "    validation_data = validation_generator,\n",
        "                    validation_steps=STEP_SIZE_VALID,\n",
        "                    epochs=EPOCHS,verbose=1,\n",
        "                    callbacks=[cp_callback, earlystop_callback, reduce_lr])\n",
        "#steps_for_epoch\n",
        "print(history.history)\n",
        "\n",
        "def plot_graphs(history, string):\n",
        "    plt.plot(history.history[string])\n",
        "    plt.plot(history.history['val_'+string], '')\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(string)\n",
        "    plt.legend([string, 'val_'+string])\n",
        "    plt.show()\n",
        "\n",
        "plot_graphs(history, 'dense_accuracy')\n",
        "\n",
        "plot_graphs(history, 'dense_1_accuracy')\n",
        "\n",
        "plot_graphs(history, 'dense_loss')\n",
        "\n",
        "plot_graphs(history, 'dense_1_loss')\n",
        "\n",
        "plot_graphs(history, 'dense_2_loss')\n",
        "\n",
        "plot_graphs(history, 'dense_f1score')\n",
        "\n",
        "plot_graphs(history, 'dense_1_f1score')\n",
        "\n",
        "## Submission\n",
        "\n",
        "## To load a best weights from a saved file (.h5)\n",
        "model.load_weights(checkpoint_path)\n",
        "\n",
        "test_df.head()\n",
        "\n",
        "total_number = len(test_df[\"ID\"])\n",
        "total_number\n",
        "\n",
        "test_df[\"ID\"] = test_df[\"ID\"].apply(add_png)\n",
        "test_df.head()\n",
        "\n",
        "test_datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "        rescale=1./255)\n",
        "\n",
        "test_generator=test_datagen.flow_from_dataframe(\n",
        "  dataframe=test_df,\n",
        "  directory=test_img_path,\n",
        "  x_col=\"ID\",\n",
        "  y_col=None,\n",
        "  # subset=\"validation\",\n",
        "  batch_size=BATCH_SIZE,\n",
        "  seed=seed,\n",
        "  shuffle=False,\n",
        "  class_mode=None,\n",
        "  target_size=(img_size,img_size))\n",
        "\n",
        "STEP_SIZE_TEST=test_generator.n//test_generator.batch_size\n",
        "test_generator.reset()\n",
        "pred=model.predict_generator(test_generator,\n",
        "  steps=200,\n",
        "  verbose=1)\n",
        "\n",
        "pred\n",
        "\n",
        "len(pred[0])\n",
        "\n",
        "prob1 = pred[0][0]\n",
        "prob2 = pred[1][0]\n",
        "prob3 = pred[2][0]\n",
        "prob1_np = np.array(prob1)\n",
        "prob2_np = np.array(prob2)\n",
        "prob3_np = np.array(prob3)\n",
        "# print(prob1, prob2, prob3)\n",
        "probs2 = np.mean([prob1_np, prob2_np, prob3_np], axis=0)\n",
        "probs2\n",
        "\n",
        "answer = np.argmax(probs2, axis=-1)\n",
        "answer\n",
        "\n",
        "# probs = model.predict(ds_testAug, verbose=1, steps=steps * cfg['tta_steps'])\n",
        "probs = np.stack(pred)\n",
        "# probs = probs[:, :count_data_items(filenames_test) * cfg['tta_steps']]\n",
        "# probs = probs[:, :test_generator.n * STEP_SIZE_TEST]\n",
        "# probs = np.stack(np.split(probs, STEP_SIZE_TEST, axis=1), axis=1)\n",
        "# probs = np.mean(probs, axis=1)\n",
        "\n",
        "probs2 = probs[:, :-1]\n",
        "\n",
        "len(probs2)\n",
        "\n",
        "prob_re = probs.reshape(5698,3,5)\n",
        "\n",
        "probs2 = np.mean(prob_re, axis=1)\n",
        "\n",
        "len(probs2)\n",
        "\n",
        "from tqdm import tqdm\n",
        "targetlist = []\n",
        "for i in tqdm(range(len(pred[0]))):\n",
        "    prob1 = pred[0][i]\n",
        "    prob2 = pred[1][i]\n",
        "    prob3 = pred[2][i]\n",
        "    prob1_np = np.array(prob1)\n",
        "    prob2_np = np.array(prob2)\n",
        "    prob3_np = np.array(prob3)\n",
        "    probs2 = np.mean([prob1_np, prob2_np, prob3_np], axis=0)\n",
        "    answer = np.argmax(probs2, axis=0)\n",
        "    targetlist.append(answer)\n",
        "len(targetlist)\n",
        "\n",
        "test_df['Target'] = targetlist\n",
        "test_df.head()\n",
        "\n",
        "len(targetlist)\n",
        "\n",
        "def remove_png(name):\n",
        "    removed = name.split(\".\")\n",
        "    name = removed[0]\n",
        "    return name\n",
        "\n",
        "test_df[\"ID\"] = test_df[\"ID\"].apply(remove_png)\n",
        "test_df.head()\n",
        "\n",
        "target_dic = {0:'10_콘크리트외벽', 1:'20_조적외벽', 2:'30_판넬외벽', 3:'40_유리외벽', 4:'50_기타외벽'}\n",
        "\n",
        "submit_df = test_df.replace({'Target':target_dic})\n",
        "submit_df.head()\n",
        "\n",
        "submit_df['Target'].value_counts()\n",
        "\n",
        "from nipa.taskSubmit import nipa_submit\n",
        "\n",
        "team_id = \"1429\"\n",
        "task_no= \"148\"\n",
        "prediction_path = '/home/workspace/user-workspace/prediction/prediction148.csv'\n",
        "save_path = os.path.join(DATA_OUT_PATH, model_name, 'prediction148.csv')\n",
        "\n",
        "submit_df.to_csv(prediction_path, index = False, header = True)\n",
        "submit_df.to_csv(save_path, index = False, header = True)\n",
        "\n",
        "print(\"is file: \", os.path.isfile(prediction_path))\n",
        "\n",
        "nipa_submit(team_id=team_id,\n",
        "task_no=task_no,\n",
        "result=prediction_path\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}